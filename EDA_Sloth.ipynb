{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T09:12:45.400815Z",
     "start_time": "2025-06-02T09:12:10.730353Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "df = read_csv(\"../data/data_collection_202412_202412.csv\", low_memory=False, encoding='latin1', delimiter=';')"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:12:45.428153Z",
     "start_time": "2025-06-02T09:12:45.426364Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "76a153a902ea9ba0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:13:00.125552Z",
     "start_time": "2025-06-02T09:12:45.496099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 1: Remove low variance columns (protect 'Target')\n",
    "def remove_low_variance_columns(df, variance_threshold=0.01, protected_cols=['Target']):\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in protected_cols]\n",
    "\n",
    "    if len(numeric_cols) > 0:\n",
    "        variances = df[numeric_cols].var()\n",
    "        low_variance_cols = variances[variances < variance_threshold].index.tolist()\n",
    "    else:\n",
    "        low_variance_cols = []\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    cat_cols = [col for col in cat_cols if col not in protected_cols]\n",
    "    dominant_cat_cols = []\n",
    "\n",
    "    for col in cat_cols:\n",
    "        if len(df[col].value_counts()) > 0:\n",
    "            dominant_pct = df[col].value_counts(normalize=True).iloc[0]\n",
    "            if dominant_pct > 0.95:\n",
    "                dominant_cat_cols.append(col)\n",
    "\n",
    "    return low_variance_cols, dominant_cat_cols\n",
    "\n",
    "\n",
    "# Run cleaning\n",
    "low_var_numeric, dominant_cat = remove_low_variance_columns(df, protected_cols=['Target'])\n",
    "columns_to_drop_step1 = low_var_numeric + dominant_cat\n",
    "df_step1 = df.drop(columns=columns_to_drop_step1)\n",
    "\n",
    "# Step 2: Remove high missing (protect 'Target')\n",
    "missing_summary = pd.DataFrame({\n",
    "    'missing_count': df_step1.isnull().sum(),\n",
    "    'missing_percentage': (df_step1.isnull().sum() / len(df_step1)) * 100\n",
    "})\n",
    "\n",
    "high_missing_cols = missing_summary[\n",
    "    (missing_summary['missing_percentage'] > 97) &\n",
    "    (missing_summary.index != 'Target')\n",
    "    ].index.tolist()\n",
    "\n",
    "df_cleaned = df_step1.drop(columns=high_missing_cols)\n",
    "\n",
    "print(f\"FINAL RESULT:\")\n",
    "print(f\"Original columns: {df.shape[1]}\")\n",
    "print(f\"After cleaning: {df_cleaned.shape[1]}\")\n",
    "print(f\"Removed: {df.shape[1] - df_cleaned.shape[1]} columns\")\n",
    "print(f\"Target preserved: {'Target' in df_cleaned.columns}\")"
   ],
   "id": "ea34f7a83a10cb10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT:\n",
      "Original columns: 918\n",
      "After cleaning: 557\n",
      "Removed: 361 columns\n",
      "Target preserved: True\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:13:29.358793Z",
     "start_time": "2025-06-02T09:13:00.202106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify and fix other numeric columns stored as strings\n",
    "print(\"Fixing all numeric columns stored as strings...\")\n",
    "\n",
    "# Find columns that should be numeric but are stored as objects\n",
    "numeric_pattern_cols = [col for col in df_cleaned.columns if\n",
    "                        any(prefix in col for prefix in ['QTD_', 'VAL_', 'TAX_', 'PCT_', 'IND_'])]\n",
    "\n",
    "for col in numeric_pattern_cols:\n",
    "    if df_cleaned[col].dtype == 'object':\n",
    "        print(f\"Converting {col} to numeric...\")\n",
    "        # Handle comma decimal separators\n",
    "        df_cleaned[col] = df_cleaned[col].astype(str).str.replace(',', '.')\n",
    "        df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
    "\n",
    "print(\"Data type conversion completed!\")"
   ],
   "id": "3a207d0cbf60697c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing all numeric columns stored as strings...\n",
      "Converting IND_T_S_GLOBAL to numeric...\n",
      "Converting IND_PSIN_PCOL_ENI to numeric...\n",
      "Converting QTD_ANOS_ANTIG_CLIENTE to numeric...\n",
      "Converting VAL_MARG_TECNICA_MED_PREF to numeric...\n",
      "Converting TAX_SINISTR_MED_PREF to numeric...\n",
      "Converting TAX_CAPTACAO_MED_PREF to numeric...\n",
      "Converting TAX_ANULACAO_MED_PREF to numeric...\n",
      "Converting VAL_IND_COMPET_MED_PREF to numeric...\n",
      "Converting PCT_FAM_MAIS_APL_MED_PREF to numeric...\n",
      "Converting PCT_FAM_MA_PCA_MED_PREF to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG_AP to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG_ATCP to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG_AUTO to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG_CAP to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG_MRH to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG_PPR to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG_SD to numeric...\n",
      "Converting VAL_MAX_MATUR_VIG_VDR to numeric...\n",
      "Converting VAL_PCA_VIG_ENTRE_12_24M_SFIN to numeric...\n",
      "Converting VAL_PCA_VIG_ENTRE_24_36M_SFIN to numeric...\n",
      "Converting VAL_PCA_VIG_ENTRE_36_48M_SFIN to numeric...\n",
      "Converting VAL_PCA_VIG_ENTRE_48_72M_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_VIG_MAIS_72M_SFIN to numeric...\n",
      "Converting DSC_IND_CHURN_AP to numeric...\n",
      "Converting DSC_IND_CHURN_AUTO to numeric...\n",
      "Converting DSC_IND_CHURN_CAP to numeric...\n",
      "Converting DSC_IND_CHURN_MRH to numeric...\n",
      "Converting DSC_IND_CHURN_PPR to numeric...\n",
      "Converting DSC_IND_CHURN_SD to numeric...\n",
      "Converting DSC_IND_CHURN_VDR to numeric...\n",
      "Converting VAL_PCA_APL_VIG_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_VIG_AP to numeric...\n",
      "Converting VAL_PCA_APL_VIG_ATCP to numeric...\n",
      "Converting VAL_PCA_APL_VIG_AUTO to numeric...\n",
      "Converting VAL_PCA_APL_VIG_CAP to numeric...\n",
      "Converting VAL_PCA_APL_VIG_MRH to numeric...\n",
      "Converting VAL_PCA_APL_VIG_PPR to numeric...\n",
      "Converting VAL_PCA_APL_VIG_SD to numeric...\n",
      "Converting VAL_PCA_APL_VIG_VDR to numeric...\n",
      "Converting VAL_SLD_PAG_SIN_ABERT to numeric...\n",
      "Converting VAL_INDEMN_SIN_ABERT to numeric...\n",
      "Converting VAL_SLD_PAG_SIN_FEC_UMES to numeric...\n",
      "Converting VAL_SLD_PAG_SIN_FEC_U12M to numeric...\n",
      "Converting VAL_SLD_PAG_SIN_FEC_U36M to numeric...\n",
      "Converting VAL_INDEMN_SIN_FEC_UMES to numeric...\n",
      "Converting VAL_INDEMN_SIN_FEC_U12M to numeric...\n",
      "Converting VAL_INDEMN_SIN_FEC_U36M to numeric...\n",
      "Converting VAL_PCA_APL_ANU_U3M_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_ANU_U6M_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_ANU_U9M_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_ANU_U12M_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_ANU_U36M_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_ANU_U12M_AUTO to numeric...\n",
      "Converting PCT_QTD_PG_DEB_DIR to numeric...\n",
      "Converting PCT_QTD_PG_MED to numeric...\n",
      "Converting PCT_QTD_PG_ATM to numeric...\n",
      "Converting PCT_QTD_PG_OTHERS to numeric...\n",
      "Converting PCT_APL_VIG_FRAC_PG_MES to numeric...\n",
      "Converting PCT_APL_VIG_FRAC_PG_TRI to numeric...\n",
      "Converting PCT_APL_VIG_FRAC_PG_SEM to numeric...\n",
      "Converting PCT_APL_VIG_FRAC_PG_ANU to numeric...\n",
      "Converting PCT_APL_VIG_FRAC_UNICO to numeric...\n",
      "Converting VAL_PENSOES_SS_DICO to numeric...\n",
      "Converting VAL_CUST_HABITACAO_DICO to numeric...\n",
      "Converting TAX_SINISTR_3A to numeric...\n",
      "Converting VAL_MARG_TECNICA_3A to numeric...\n",
      "Converting VAL_CUSTO_SIN_EX_ABR to numeric...\n",
      "Converting VAL_CUSTO_SIN_EX_FEC to numeric...\n",
      "Converting VAL_DENS_POP_CONC_INE to numeric...\n",
      "Converting VAL_MAX_MATUR_ANU to numeric...\n",
      "Converting VAL_MAX_MATUR_ANU_AUTO to numeric...\n",
      "Converting VAL_MAX_MATUR_ANU_CAP to numeric...\n",
      "Converting VAL_MAX_MATUR_ANU_MRH to numeric...\n",
      "Converting VAL_MAX_MATUR_ANU_PPR to numeric...\n",
      "Converting VAL_MAX_MATUR_ANU_SD to numeric...\n",
      "Converting VAL_MIN_MATUR_ANU to numeric...\n",
      "Converting VAL_MIN_MATUR_ANU_AUTO to numeric...\n",
      "Converting VAL_MIN_MATUR_ANU_CAP to numeric...\n",
      "Converting VAL_MIN_MATUR_ANU_MRH to numeric...\n",
      "Converting VAL_MIN_MATUR_ANU_PPR to numeric...\n",
      "Converting VAL_MIN_MATUR_ANU_SD to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG_AP to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG_ATCP to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG_AUTO to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG_CAP to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG_MRH to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG_PPR to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG_SD to numeric...\n",
      "Converting VAL_MIN_MATUR_VIG_VDR to numeric...\n",
      "Converting VAL_PCA_APL_ANU to numeric...\n",
      "Converting VAL_PCA_APL_ANU_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_ANU_AP to numeric...\n",
      "Converting VAL_PCA_APL_ANU_AUTO to numeric...\n",
      "Converting VAL_PCA_APL_ANU_CAP to numeric...\n",
      "Converting VAL_PCA_APL_ANU_MRH to numeric...\n",
      "Converting VAL_PCA_APL_ANU_PPR to numeric...\n",
      "Converting VAL_PCA_APL_ANU_SD to numeric...\n",
      "Converting VAL_PCA_VIG_ENTRE_6M_12M_SFIN to numeric...\n",
      "Converting VAL_PCA_APL_VIG_MENOS_6M_SFIN to numeric...\n",
      "Converting VAL_PROV_INI_SIN_FEC to numeric...\n",
      "Converting VAL_REF_PROV_SIN_FEC to numeric...\n",
      "Converting VAL_RED_PROV_SIN_FEC to numeric...\n",
      "Converting PCT_TOTAL_UTENTES_MDF to numeric...\n",
      "Converting PCT_TOTAL_SEM_MDF to numeric...\n",
      "Converting PCT_SEM_MDF_OPCAO to numeric...\n",
      "Data type conversion completed!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:13:44.260255Z",
     "start_time": "2025-06-02T09:13:29.452518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Step 1: Remove low variance columns (protect 'Target')\n",
    "def remove_low_variance_columns(df, variance_threshold=0.01, protected_cols=['Target']):\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in protected_cols]\n",
    "\n",
    "    if len(numeric_cols) > 0:\n",
    "        variances = df[numeric_cols].var()\n",
    "        low_variance_cols = variances[variances < variance_threshold].index.tolist()\n",
    "    else:\n",
    "        low_variance_cols = []\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    cat_cols = [col for col in cat_cols if col not in protected_cols]\n",
    "    dominant_cat_cols = []\n",
    "\n",
    "    for col in cat_cols:\n",
    "        if len(df[col].value_counts()) > 0:\n",
    "            dominant_pct = df[col].value_counts(normalize=True).iloc[0]\n",
    "            if dominant_pct > 0.95:\n",
    "                dominant_cat_cols.append(col)\n",
    "\n",
    "    return low_variance_cols, dominant_cat_cols\n",
    "\n",
    "\n",
    "# Run cleaning\n",
    "low_var_numeric, dominant_cat = remove_low_variance_columns(df, protected_cols=['Target'])\n",
    "columns_to_drop_step1 = low_var_numeric + dominant_cat\n",
    "df_step1 = df.drop(columns=columns_to_drop_step1)\n",
    "\n",
    "# Step 2: Remove high missing (protect 'Target')\n",
    "missing_summary = pd.DataFrame({\n",
    "    'missing_count': df_step1.isnull().sum(),\n",
    "    'missing_percentage': (df_step1.isnull().sum() / len(df_step1)) * 100\n",
    "})\n",
    "\n",
    "high_missing_cols = missing_summary[\n",
    "    (missing_summary['missing_percentage'] > 97) &\n",
    "    (missing_summary.index != 'Target')\n",
    "    ].index.tolist()\n",
    "\n",
    "df_cleaned = df_step1.drop(columns=high_missing_cols)\n",
    "\n",
    "print(f\"FINAL RESULT:\")\n",
    "print(f\"Original columns: {df.shape[1]}\")\n",
    "print(f\"After cleaning: {df_cleaned.shape[1]}\")\n",
    "print(f\"Removed: {df.shape[1] - df_cleaned.shape[1]} columns\")\n",
    "print(f\"Target preserved: {'Target' in df_cleaned.columns}\")"
   ],
   "id": "fefb94a35a359509",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT:\n",
      "Original columns: 918\n",
      "After cleaning: 557\n",
      "Removed: 361 columns\n",
      "Target preserved: True\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:13:44.338172Z",
     "start_time": "2025-06-02T09:13:44.332911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"TARGET in original df: {'TARGET' in df.columns}\")\n",
    "print(f\"TARGET in cleaned df: {'TARGET' in df_cleaned.columns}\")\n",
    "\n",
    "# If TARGET was accidentally removed, let's check why\n",
    "if 'Target' in df.columns and 'Target' not in df_cleaned.columns:\n",
    "    print(\"TARGET was accidentally removed during cleaning!\")\n",
    "    print(\"Let's check TARGET's characteristics:\")\n",
    "    print(f\"TARGET missing %: {(df['TARGET'].isnull().sum() / len(df)) * 100:.2f}%\")\n",
    "    if df['Target'].dtype in ['int64', 'float64']:\n",
    "        print(f\"TARGET variance: {df['TARGET'].var()}\")"
   ],
   "id": "6ca2a4749962f1c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET in original df: False\n",
      "TARGET in cleaned df: False\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:13:44.399324Z",
     "start_time": "2025-06-02T09:13:44.390275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Target Analysis\n",
    "print(\"\\nSTEP 3 - Target Variable Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Target distribution:\")\n",
    "print(df_cleaned['Target'].value_counts())\n",
    "print(\"\\nTarget percentages:\")\n",
    "target_dist = df_cleaned['Target'].value_counts(normalize=True)\n",
    "print(target_dist)\n",
    "print(f\"\\nMySavings cross-sell rate: {target_dist[1]:.2%}\")"
   ],
   "id": "d05fbbf629e739c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3 - Target Variable Analysis:\n",
      "========================================\n",
      "Target distribution:\n",
      "Target\n",
      "0    377206\n",
      "1       716\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target percentages:\n",
      "Target\n",
      "0    0.998105\n",
      "1    0.001895\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "MySavings cross-sell rate: 0.19%\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:13:51.249639Z",
     "start_time": "2025-06-02T09:13:44.444778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 4 - Duplicate Check:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df_cleaned.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for duplicate customer IDs\n",
    "duplicate_customers = df_cleaned['ID_CLIENTE_ANON'].duplicated().sum()\n",
    "print(f\"Duplicate customer IDs: {duplicate_customers}\")\n",
    "\n",
    "# If duplicates exist, remove them\n",
    "if duplicate_rows > 0:\n",
    "    print(\"Removing duplicate rows...\")\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"Rows after duplicate removal: {df_cleaned.shape[0]}\")\n",
    "\n",
    "if duplicate_customers > 0:\n",
    "    print(\"Removing duplicate customers (keeping first occurrence)...\")\n",
    "    df_cleaned = df_cleaned.drop_duplicates(subset=['ID_CLIENTE_ANON'], keep='first')\n",
    "    print(f\"Rows after customer dedup: {df_cleaned.shape[0]}\")"
   ],
   "id": "a998316cee558940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4 - Duplicate Check:\n",
      "========================================\n",
      "Duplicate rows: 0\n",
      "Duplicate customer IDs: 34\n",
      "Removing duplicate customers (keeping first occurrence)...\n",
      "Rows after customer dedup: 377888\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:24:27.210276Z",
     "start_time": "2025-06-02T09:24:27.171682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 5 - Data Type Fixes and Quality Check:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check data types of key columns\n",
    "print(\"Key column data types:\")\n",
    "key_cols = ['QTD_IDADE', 'QTD_ANOS_ANTIG_CLIENTE', 'Target']\n",
    "for col in key_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        print(f\"  {col}: {df_cleaned[col].dtype}\")\n",
    "\n",
    "# Fix numeric columns that are stored as strings\n",
    "print(f\"\\nFixing data types...\")\n",
    "\n",
    "# Age should be numeric\n",
    "if df_cleaned['QTD_IDADE'].dtype == 'object':\n",
    "    print(\"Converting QTD_IDADE to numeric...\")\n",
    "    df_cleaned['QTD_IDADE'] = pd.to_numeric(df_cleaned['QTD_IDADE'], errors='coerce')\n",
    "\n",
    "# Customer tenure should be numeric\n",
    "if df_cleaned['QTD_ANOS_ANTIG_CLIENTE'].dtype == 'object':\n",
    "    print(\"Converting QTD_ANOS_ANTIG_CLIENTE to numeric...\")\n",
    "    # Handle potential comma as decimal separator\n",
    "    df_cleaned['QTD_ANOS_ANTIG_CLIENTE'] = df_cleaned['QTD_ANOS_ANTIG_CLIENTE'].astype(str).str.replace(',', '.')\n",
    "    df_cleaned['QTD_ANOS_ANTIG_CLIENTE'] = pd.to_numeric(df_cleaned['QTD_ANOS_ANTIG_CLIENTE'], errors='coerce')\n",
    "\n",
    "# Now check the data quality\n",
    "print(f\"\\nAge distribution:\")\n",
    "print(df_cleaned['QTD_IDADE'].describe())\n",
    "age_issues = (df_cleaned['QTD_IDADE'] < 18) | (df_cleaned['QTD_IDADE'] > 80)\n",
    "print(f\"Ages outside 18-80 range: {age_issues.sum()}\")\n",
    "\n",
    "print(f\"\\nCustomer tenure distribution:\")\n",
    "print(df_cleaned['QTD_ANOS_ANTIG_CLIENTE'].describe())\n",
    "tenure_issues = df_cleaned['QTD_ANOS_ANTIG_CLIENTE'] < 0\n",
    "print(f\"Negative tenure values: {tenure_issues.sum()}\")\n",
    "\n",
    "# Check for NaN values created during conversion\n",
    "print(f\"\\nNaN values created during conversion:\")\n",
    "print(f\"  QTD_IDADE: {df_cleaned['QTD_IDADE'].isnull().sum()}\")\n",
    "print(f\"  QTD_ANOS_ANTIG_CLIENTE: {df_cleaned['QTD_ANOS_ANTIG_CLIENTE'].isnull().count()}\")"
   ],
   "id": "31b52ea40a704be7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5 - Data Type Fixes and Quality Check:\n",
      "========================================\n",
      "Key column data types:\n",
      "  QTD_IDADE: int64\n",
      "  QTD_ANOS_ANTIG_CLIENTE: float64\n",
      "  Target: int64\n",
      "\n",
      "Fixing data types...\n",
      "\n",
      "Age distribution:\n",
      "count    377888.000000\n",
      "mean         48.170458\n",
      "std          13.082849\n",
      "min          19.000000\n",
      "25%          38.000000\n",
      "50%          48.000000\n",
      "75%          57.000000\n",
      "max          79.000000\n",
      "Name: QTD_IDADE, dtype: float64\n",
      "Ages outside 18-80 range: 0\n",
      "\n",
      "Customer tenure distribution:\n",
      "count    372054.000000\n",
      "mean         16.516462\n",
      "std          10.968809\n",
      "min          -0.080000\n",
      "25%           6.830000\n",
      "50%          15.750000\n",
      "75%          24.330000\n",
      "max         112.080000\n",
      "Name: QTD_ANOS_ANTIG_CLIENTE, dtype: float64\n",
      "Negative tenure values: 3\n",
      "\n",
      "NaN values created during conversion:\n",
      "  QTD_IDADE: 0\n",
      "  QTD_ANOS_ANTIG_CLIENTE: 377888\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:27:06.032872Z",
     "start_time": "2025-06-02T09:27:04.278301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 6 - Final Dataset Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Features available: {df_cleaned.shape[1] - 1}\")\n",
    "print(f\"MySavings cross-sell rate: {df_cleaned['Target'].mean():.2%}\")\n",
    "\n",
    "# Data types after cleaning\n",
    "print(f\"\\nData types distribution:\")\n",
    "print(df_cleaned.dtypes.value_counts())\n",
    "\n",
    "# Quick missing values check\n",
    "missing_summary = (df_cleaned.isnull().sum() / len(df_cleaned) * 100).sort_values(ascending=False)\n",
    "cols_with_missing = missing_summary[missing_summary > 0]\n",
    "print(f\"\\nColumns with missing values: {len(cols_with_missing)}\")\n",
    "if len(cols_with_missing) > 0:\n",
    "    print(\"Top 5 columns with missing values:\")\n",
    "    print(cols_with_missing.head())"
   ],
   "id": "42142e94bc022e7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6 - Final Dataset Summary:\n",
      "========================================\n",
      "Dataset shape: (377888, 557)\n",
      "Features available: 556\n",
      "MySavings cross-sell rate: 0.19%\n",
      "\n",
      "Data types distribution:\n",
      "int64      196\n",
      "float64    189\n",
      "object     172\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns with missing values: 336\n",
      "Top 5 columns with missing values:\n",
      "VAL_MAX_MATUR_VIG_ATCP    96.846155\n",
      "VAL_PCA_APL_VIG_ATCP      96.846155\n",
      "VAL_MIN_MATUR_VIG_ATCP    96.846155\n",
      "QTD_MESES_CHURN_AUTO      96.592641\n",
      "VAL_MIN_MATUR_ANU_CAP     96.240685\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:27:32.855519Z",
     "start_time": "2025-06-02T09:27:32.833565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 7 - Target Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Target distribution\n",
    "target_counts = df_cleaned['Target'].value_counts()\n",
    "target_pct = df_cleaned['Target'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Target distribution:\")\n",
    "print(f\"  No MySavings (0): {target_counts[0]:,} ({target_pct[0]:.1%})\")\n",
    "print(f\"  Bought MySavings (1): {target_counts[1]:,} ({target_pct[1]:.1%})\")\n",
    "\n",
    "# Check if dataset is imbalanced (important for modeling)\n",
    "if target_pct[1] < 0.05:\n",
    "    print(\"⚠️  Highly imbalanced dataset - will need special handling in modeling\")\n",
    "elif target_pct[1] < 0.20:\n",
    "    print(\"⚠️  Imbalanced dataset - consider this in modeling approach\")\n",
    "else:\n",
    "    print(\"✓ Reasonably balanced dataset\")"
   ],
   "id": "55f628ebefde21cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7 - Target Analysis:\n",
      "========================================\n",
      "Target distribution:\n",
      "  No MySavings (0): 377,185 (99.8%)\n",
      "  Bought MySavings (1): 703 (0.2%)\n",
      "⚠️  Highly imbalanced dataset - will need special handling in modeling\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:27:47.454940Z",
     "start_time": "2025-06-02T09:27:47.353756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 8 - Demographics vs Target:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Age analysis\n",
    "print(\"Age analysis:\")\n",
    "age_stats = df_cleaned.groupby('Target')['QTD_IDADE'].agg(['mean', 'median', 'std'])\n",
    "print(age_stats)\n",
    "\n",
    "# Customer tenure analysis\n",
    "print(f\"\\nCustomer tenure analysis:\")\n",
    "tenure_stats = df_cleaned.groupby('Target')['QTD_ANOS_ANTIG_CLIENTE'].agg(['mean', 'median', 'std'])\n",
    "print(tenure_stats)\n",
    "\n",
    "# Gender analysis (if available)\n",
    "if 'DSC_SEXO' in df_cleaned.columns:\n",
    "    print(f\"\\nGender vs Target:\")\n",
    "    gender_crosstab = pd.crosstab(df_cleaned['DSC_SEXO'], df_cleaned['Target'], normalize='index')\n",
    "    print(gender_crosstab)"
   ],
   "id": "2a31e8312ad102b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 8 - Demographics vs Target:\n",
      "========================================\n",
      "Age analysis:\n",
      "             mean  median        std\n",
      "Target                              \n",
      "0       48.174148    48.0  13.082867\n",
      "1       46.190612    45.0  12.931138\n",
      "\n",
      "Customer tenure analysis:\n",
      "             mean  median        std\n",
      "Target                              \n",
      "0       16.516612   15.75  10.969189\n",
      "1       16.436408   15.71  10.771283\n",
      "\n",
      "Gender vs Target:\n",
      "Target              0         1\n",
      "DSC_SEXO                       \n",
      "F-FEMININO   0.998112  0.001888\n",
      "M-MASCULINO  0.998161  0.001839\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:32:09.069125Z",
     "start_time": "2025-06-02T09:32:09.053467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 9 - Feature Engineering for Imbalanced Cross-sell:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First, let's identify feature categories we have\n",
    "feature_categories = {}\n",
    "\n",
    "# Demographic features\n",
    "demo_cols = [col for col in df_cleaned.columns if\n",
    "             any(x in col.upper() for x in ['IDADE', 'SEXO', 'CIVIL', 'PROF', 'HABILITACOES'])]\n",
    "feature_categories['Demographics'] = demo_cols\n",
    "\n",
    "# Geographic features\n",
    "geo_cols = [col for col in df_cleaned.columns if\n",
    "            any(x in col.upper() for x in ['DISTRITO', 'CONCELHO', 'NUT', 'FREGUESIA'])]\n",
    "feature_categories['Geographic'] = geo_cols\n",
    "\n",
    "# Digital behavior\n",
    "digital_cols = [col for col in df_cleaned.columns if\n",
    "                any(x in col.upper() for x in ['LOGIN', 'APP', 'DIGITAL', 'INTERACT'])]\n",
    "feature_categories['Digital'] = digital_cols\n",
    "\n",
    "# Insurance products/policies\n",
    "insurance_cols = [col for col in df_cleaned.columns if\n",
    "                  any(x in col.upper() for x in ['APL', 'VIG', 'SEGUROS', 'APOLICE'])]\n",
    "feature_categories['Insurance'] = insurance_cols\n",
    "\n",
    "# Financial/Value features\n",
    "financial_cols = [col for col in df_cleaned.columns if\n",
    "                  any(x in col.upper() for x in ['VAL', 'CLV', 'SCORE', 'MARGEM', 'PCA'])]\n",
    "feature_categories['Financial'] = financial_cols\n",
    "\n",
    "print(\"Available feature categories:\")\n",
    "for category, cols in feature_categories.items():\n",
    "    available = [col for col in cols if col in df_cleaned.columns and col != 'Target']\n",
    "    print(f\"  {category}: {len(available)} features\")\n",
    "    if len(available) > 0:\n",
    "        print(f\"    Examples: {available[:3]}\")"
   ],
   "id": "9188d43e5bd219d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 9 - Feature Engineering for Imbalanced Cross-sell:\n",
      "==================================================\n",
      "Available feature categories:\n",
      "  Demographics: 7 features\n",
      "    Examples: ['DSC_ESTADO_CIVIL', 'DSC_SEXO', 'DSC_SITUACAO_PROF']\n",
      "  Geographic: 8 features\n",
      "    Examples: ['DSC_CONCELHO', 'DSC_DISTRITO', 'COD_NUT2']\n",
      "  Digital: 62 features\n",
      "    Examples: ['IND_LOGIN_APP_MYFID', 'IND_LOGIN_APP_DRIVE', 'IND_LOGIN_APP_MYMLT']\n",
      "  Insurance: 176 features\n",
      "    Examples: ['DSC_FAM_MAIS_APL_MED_PREF', 'PCT_FAM_MAIS_APL_MED_PREF', 'QTD_APL_VIG_T_S']\n",
      "  Financial: 89 features\n",
      "    Examples: ['VAL_MARG_TECNICA_MED_PREF', 'VAL_IND_COMPET_MED_PREF', 'DSC_FAM_MA_PCA_MED_PREF']\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:32:23.477704Z",
     "start_time": "2025-06-02T09:32:22.549771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 10 - Age Feature Engineering:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "\n",
    "# Create age groups (common in insurance cross-sell)\n",
    "def create_age_groups(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create age bins\n",
    "    age_bins = [18, 25, 35, 45, 55, 65, 80]\n",
    "    age_labels = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "\n",
    "    df['Age_Group'] = pd.cut(df['QTD_IDADE'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_engineered = create_age_groups(df_cleaned)\n",
    "\n",
    "# Check age group distribution vs target\n",
    "print(\"Age groups vs Target rate:\")\n",
    "age_target_analysis = df_engineered.groupby('Age_Group')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "age_target_analysis.columns = ['Total_Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "print(age_target_analysis)\n",
    "\n",
    "# Find best performing age groups\n",
    "best_age_groups = age_target_analysis.sort_values('Conversion_Rate', ascending=False)\n",
    "print(f\"\\nBest converting age groups:\")\n",
    "print(best_age_groups.head(3))"
   ],
   "id": "8c82877e7e591dc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 10 - Age Feature Engineering:\n",
      "========================================\n",
      "Age groups vs Target rate:\n",
      "           Total_Customers  MySavings_Buyers  Conversion_Rate\n",
      "Age_Group                                                    \n",
      "18-24                 5701                21           0.0037\n",
      "25-34                59409               122           0.0021\n",
      "35-44                87964               189           0.0021\n",
      "45-54               108110               167           0.0015\n",
      "55-64                68871               142           0.0021\n",
      "65+                  47833                62           0.0013\n",
      "\n",
      "Best converting age groups:\n",
      "           Total_Customers  MySavings_Buyers  Conversion_Rate\n",
      "Age_Group                                                    \n",
      "18-24                 5701                21           0.0037\n",
      "25-34                59409               122           0.0021\n",
      "35-44                87964               189           0.0021\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:41:04.730079Z",
     "start_time": "2025-06-02T09:41:04.724009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"FEATURE ENGINEERING\")\n",
    "print('=' * 42)"
   ],
   "id": "13b389d95cf135aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE ENGINEERING\n",
      "==========================================\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:41:07.853727Z",
     "start_time": "2025-06-02T09:41:07.752967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 11 - Insurance Product Features:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find all insurance product columns\n",
    "insurance_cols = [col for col in df_engineered.columns if 'QTD_APL_VIG' in col and col != 'Target']\n",
    "print(f\"Found {len(insurance_cols)} insurance product columns\")\n",
    "\n",
    "# Show first 5 to understand the data\n",
    "print(f\"\\nFirst 5 insurance columns sample:\")\n",
    "for col in insurance_cols[:5]:\n",
    "    print(f\"  {col}:\")\n",
    "    print(f\"    Data type: {df_engineered[col].dtype}\")\n",
    "    print(f\"    Non-null values: {df_engineered[col].notna().sum()}\")\n",
    "    print(f\"    Sample values: {df_engineered[col].dropna().head(3).tolist()}\")\n",
    "    print()\n",
    "\n",
    "# Check if they're numeric and create total count\n",
    "numeric_insurance_cols = []\n",
    "for col in insurance_cols:\n",
    "    if df_engineered[col].dtype in ['int64', 'float64']:\n",
    "        numeric_insurance_cols.append(col)\n",
    "\n",
    "print(f\"Numeric insurance columns: {len(numeric_insurance_cols)}\")\n",
    "\n",
    "if len(numeric_insurance_cols) > 0:\n",
    "    # Create total insurance products\n",
    "    df_engineered['Total_Insurance_Products'] = df_engineered[numeric_insurance_cols].fillna(0).sum(axis=1)\n",
    "\n",
    "    print(f\"\\nTotal Insurance Products distribution:\")\n",
    "    print(df_engineered['Total_Insurance_Products'].describe())\n",
    "\n",
    "    # Analyze relationship with target\n",
    "    print(f\"\\nInsurance Products vs MySavings Target:\")\n",
    "    product_target = df_engineered.groupby('Total_Insurance_Products')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    product_target.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "    # Only show groups with at least 10 customers\n",
    "    product_target = product_target[product_target['Customers'] >= 10]\n",
    "    print(product_target.head(10))"
   ],
   "id": "73e886275fa076b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 11 - Insurance Product Features:\n",
      "==================================================\n",
      "Found 35 insurance product columns\n",
      "\n",
      "First 5 insurance columns sample:\n",
      "  QTD_APL_VIG_T_S:\n",
      "    Data type: int64\n",
      "    Non-null values: 377888\n",
      "    Sample values: [0, 0, 0]\n",
      "\n",
      "  QTD_APL_VIG_T_S_AP:\n",
      "    Data type: int64\n",
      "    Non-null values: 377888\n",
      "    Sample values: [0, 0, 0]\n",
      "\n",
      "  QTD_APL_VIG_T_S_ATCP:\n",
      "    Data type: int64\n",
      "    Non-null values: 377888\n",
      "    Sample values: [0, 0, 0]\n",
      "\n",
      "  QTD_APL_VIG_T_S_CAP:\n",
      "    Data type: int64\n",
      "    Non-null values: 377888\n",
      "    Sample values: [0, 0, 0]\n",
      "\n",
      "  QTD_APL_VIG_T_S_PPR:\n",
      "    Data type: int64\n",
      "    Non-null values: 377888\n",
      "    Sample values: [0, 0, 0]\n",
      "\n",
      "Numeric insurance columns: 35\n",
      "\n",
      "Total Insurance Products distribution:\n",
      "count    377888.000000\n",
      "mean         12.541129\n",
      "std          10.671509\n",
      "min           1.000000\n",
      "25%           6.000000\n",
      "50%          10.000000\n",
      "75%          16.000000\n",
      "max         477.000000\n",
      "Name: Total_Insurance_Products, dtype: float64\n",
      "\n",
      "Insurance Products vs MySavings Target:\n",
      "                          Customers  MySavings_Buyers  Conversion_Rate\n",
      "Total_Insurance_Products                                              \n",
      "2                               835                 2           0.0024\n",
      "3                             60326                61           0.0010\n",
      "4                                81                 0           0.0000\n",
      "5                              1868                 3           0.0016\n",
      "6                             59024                64           0.0011\n",
      "7                             27165                40           0.0015\n",
      "8                              1920                 1           0.0005\n",
      "9                             34887                39           0.0011\n",
      "10                            31566                68           0.0022\n",
      "11                             1186                 3           0.0025\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:45:27.582895Z",
     "start_time": "2025-06-02T09:45:27.054084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 12 - Financial Features (Properly Fixed):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fix CLV column (European format: comma as decimal)\n",
    "print(\"Fixing CLV format...\")\n",
    "df_engineered['CLV_numeric'] = df_engineered['CLV'].str.replace('.', '').str.replace(',', '.').astype(float)\n",
    "\n",
    "print(f\"CLV after conversion:\")\n",
    "print(f\"  Non-null values: {df_engineered['CLV_numeric'].notna().sum()}\")\n",
    "print(f\"  Distribution: {df_engineered['CLV_numeric'].describe()}\")\n",
    "\n",
    "# Create CLV segments\n",
    "print(f\"\\nCreating CLV segments...\")\n",
    "df_engineered['CLV_Segment'] = pd.qcut(df_engineered['CLV_numeric'],\n",
    "                                       q=4, labels=['Low', 'Medium', 'High', 'Premium'],\n",
    "                                       duplicates='drop')\n",
    "\n",
    "clv_analysis = df_engineered.groupby('CLV_Segment')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "clv_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "print(\"CLV segments vs Target:\")\n",
    "print(clv_analysis)\n",
    "\n",
    "# Use Score columns (these are already clean)\n",
    "print(f\"\\nAnalyzing Score Features:\")\n",
    "\n",
    "# CLV Score analysis\n",
    "print(\"CLV Score vs Target:\")\n",
    "clv_score_analysis = df_engineered.groupby('VAL_SCORE_CLV')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "clv_score_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "print(clv_score_analysis.sort_values('Conversion_Rate', ascending=False))\n",
    "\n",
    "# FNOL Score analysis\n",
    "print(f\"\\nFNOL Score vs Target:\")\n",
    "fnol_score_analysis = df_engineered.groupby('VAL_SCORE_FNOL')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "fnol_score_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "print(fnol_score_analysis.sort_values('Conversion_Rate', ascending=False))\n",
    "\n",
    "# Create combined financial health score\n",
    "print(f\"\\nCreating combined financial score...\")\n",
    "df_engineered['Financial_Health_Score'] = df_engineered['VAL_SCORE_CLV'] + df_engineered['VAL_SCORE_FNOL']\n",
    "\n",
    "fin_health_analysis = df_engineered.groupby('Financial_Health_Score')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "fin_health_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "print(\"Combined Financial Health Score vs Target:\")\n",
    "print(fin_health_analysis.sort_values('Conversion_Rate', ascending=False).head(10))"
   ],
   "id": "1a0940da73202cce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 12 - Financial Features (Properly Fixed):\n",
      "==================================================\n",
      "Fixing CLV format...\n",
      "CLV after conversion:\n",
      "  Non-null values: 350173\n",
      "  Distribution: count    350173.000000\n",
      "mean        392.673454\n",
      "std         691.799093\n",
      "min       -9901.920000\n",
      "25%         106.540000\n",
      "50%         320.460000\n",
      "75%         632.790000\n",
      "max        8831.480000\n",
      "Name: CLV_numeric, dtype: float64\n",
      "\n",
      "Creating CLV segments...\n",
      "CLV segments vs Target:\n",
      "             Customers  MySavings_Buyers  Conversion_Rate\n",
      "CLV_Segment                                              \n",
      "Low              87545               172           0.0020\n",
      "Medium           87542               144           0.0016\n",
      "High             87545               162           0.0019\n",
      "Premium          87541               156           0.0018\n",
      "\n",
      "Analyzing Score Features:\n",
      "CLV Score vs Target:\n",
      "               Customers  MySavings_Buyers  Conversion_Rate\n",
      "VAL_SCORE_CLV                                              \n",
      "1.0                28717                62           0.0022\n",
      "2.0                40669                82           0.0020\n",
      "6.0                74489               142           0.0019\n",
      "5.0                56519                99           0.0018\n",
      "3.0                33076                57           0.0017\n",
      "7.0                31892                53           0.0017\n",
      "4.0                84811               139           0.0016\n",
      "\n",
      "FNOL Score vs Target:\n",
      "                Customers  MySavings_Buyers  Conversion_Rate\n",
      "VAL_SCORE_FNOL                                              \n",
      "1.0                 31668                67           0.0021\n",
      "2.0                 53081               102           0.0019\n",
      "5.0                 51487               100           0.0019\n",
      "4.0                 77439               137           0.0018\n",
      "6.0                 67409               122           0.0018\n",
      "7.0                 30189                49           0.0016\n",
      "3.0                 38900                57           0.0015\n",
      "\n",
      "Creating combined financial score...\n",
      "Combined Financial Health Score vs Target:\n",
      "                        Customers  MySavings_Buyers  Conversion_Rate\n",
      "Financial_Health_Score                                              \n",
      "11.0                        16907                39           0.0023\n",
      "2.0                         27210                60           0.0022\n",
      "5.0                         16547                34           0.0021\n",
      "4.0                         31719                64           0.0020\n",
      "13.0                         4067                 8           0.0020\n",
      "10.0                        33567                61           0.0018\n",
      "12.0                        59594               107           0.0018\n",
      "8.0                         52673                92           0.0017\n",
      "7.0                         26390                41           0.0016\n",
      "9.0                         25252                41           0.0016\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T09:46:31.166541Z",
     "start_time": "2025-06-02T09:46:31.100310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 13 - Digital Behavior Features:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find digital behavior columns\n",
    "digital_cols = [col for col in df_engineered.columns if\n",
    "                any(x in col.upper() for x in ['LOGIN', 'APP', 'DIGITAL', 'INTERACT', 'DAYS', 'SITE'])]\n",
    "print(f\"Found {len(digital_cols)} digital behavior columns\")\n",
    "\n",
    "# Show some key digital columns\n",
    "key_digital = ['IND_LOGIN_SITE', 'IND_LOGIN_APP_MYFID', 'QT_DAYS_LAST_LOGIN_SITE', 'QT_INTERACTS_SITE_M1']\n",
    "available_digital = [col for col in key_digital if col in df_engineered.columns]\n",
    "\n",
    "print(f\"\\nKey digital columns available: {available_digital}\")\n",
    "\n",
    "# Check login indicators\n",
    "login_indicators = [col for col in df_engineered.columns if 'IND_LOGIN' in col]\n",
    "print(f\"\\nLogin indicators found: {len(login_indicators)}\")\n",
    "\n",
    "for col in login_indicators[:5]:  # Check first 5\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Values: {df_engineered[col].value_counts()}\")\n",
    "\n",
    "# Create digital engagement score\n",
    "print(f\"\\nCreating digital engagement features...\")\n",
    "\n",
    "# Count total number of digital channels used\n",
    "digital_channels_used = 0\n",
    "for col in login_indicators:\n",
    "    if col in df_engineered.columns:\n",
    "        digital_channels_used += df_engineered[col].fillna(0)\n",
    "\n",
    "df_engineered['Digital_Channels_Count'] = digital_channels_used\n",
    "\n",
    "print(f\"Digital channels count vs Target:\")\n",
    "digital_analysis = df_engineered.groupby('Digital_Channels_Count')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "digital_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "print(digital_analysis)\n",
    "\n",
    "# Check recent login activity\n",
    "recent_login_cols = [col for col in df_engineered.columns if 'QT_DAYS_LAST_LOGIN' in col]\n",
    "print(f\"\\nRecent login columns: {len(recent_login_cols)}\")\n",
    "\n",
    "if len(recent_login_cols) > 0:\n",
    "    print(f\"Sample recent login column: {recent_login_cols[0]}\")\n",
    "    sample_col = recent_login_cols[0]\n",
    "    print(f\"  Data type: {df_engineered[sample_col].dtype}\")\n",
    "    print(f\"  Distribution: {df_engineered[sample_col].describe()}\")"
   ],
   "id": "648a0187bf8ee469",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 13 - Digital Behavior Features:\n",
      "==================================================\n",
      "Found 70 digital behavior columns\n",
      "\n",
      "Key digital columns available: ['IND_LOGIN_SITE', 'IND_LOGIN_APP_MYFID', 'QT_DAYS_LAST_LOGIN_SITE', 'QT_INTERACTS_SITE_M1']\n",
      "\n",
      "Login indicators found: 8\n",
      "\n",
      "IND_LOGIN_APP_MYFID:\n",
      "  Values: IND_LOGIN_APP_MYFID\n",
      "1    266193\n",
      "0    111695\n",
      "Name: count, dtype: int64\n",
      "\n",
      "IND_LOGIN_APP_DRIVE:\n",
      "  Values: IND_LOGIN_APP_DRIVE\n",
      "0    318793\n",
      "1     59095\n",
      "Name: count, dtype: int64\n",
      "\n",
      "IND_LOGIN_APP_MYMLT:\n",
      "  Values: IND_LOGIN_APP_MYMLT\n",
      "0    323743\n",
      "1     54145\n",
      "Name: count, dtype: int64\n",
      "\n",
      "IND_LOGIN_APP_VTL:\n",
      "  Values: IND_LOGIN_APP_VTL\n",
      "0    341682\n",
      "1     36206\n",
      "Name: count, dtype: int64\n",
      "\n",
      "IND_LOGIN_APP_MYSAV:\n",
      "  Values: IND_LOGIN_APP_MYSAV\n",
      "0    331792\n",
      "1     46096\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Creating digital engagement features...\n",
      "Digital channels count vs Target:\n",
      "                        Customers  MySavings_Buyers  Conversion_Rate\n",
      "Digital_Channels_Count                                              \n",
      "0                               5                 0           0.0000\n",
      "1                           98663               134           0.0014\n",
      "2                          131539               196           0.0015\n",
      "3                           82905               187           0.0023\n",
      "4                           42089                99           0.0024\n",
      "5                           15562                46           0.0030\n",
      "6                            5268                31           0.0059\n",
      "7                            1803                 9           0.0050\n",
      "8                              54                 1           0.0185\n",
      "\n",
      "Recent login columns: 7\n",
      "Sample recent login column: QT_DAYS_LAST_LOGIN_APP_MYFID\n",
      "  Data type: float64\n",
      "  Distribution: count    266193.000000\n",
      "mean        268.436773\n",
      "std         238.187840\n",
      "min           0.000000\n",
      "25%          17.000000\n",
      "50%         289.000000\n",
      "75%         359.000000\n",
      "max         975.000000\n",
      "Name: QT_DAYS_LAST_LOGIN_APP_MYFID, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:00:27.909655Z",
     "start_time": "2025-06-02T10:00:27.745225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 14 - Digital Engagement Segmentation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Create digital engagement segments based on channels used\n",
    "def create_digital_segments(channels_count):\n",
    "    if channels_count == 0:\n",
    "        return 'No_Digital'\n",
    "    elif channels_count <= 2:\n",
    "        return 'Low_Digital'\n",
    "    elif channels_count <= 4:\n",
    "        return 'Medium_Digital'\n",
    "    else:\n",
    "        return 'High_Digital'\n",
    "\n",
    "\n",
    "df_engineered['Digital_Engagement'] = df_engineered['Digital_Channels_Count'].apply(create_digital_segments)\n",
    "\n",
    "print(\"Digital engagement segments vs Target:\")\n",
    "digital_seg_analysis = df_engineered.groupby('Digital_Engagement')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "digital_seg_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "print(digital_seg_analysis.sort_values('Conversion_Rate', ascending=False))\n",
    "\n",
    "# Create recency segments for main app (MyFid)\n",
    "print(f\"\\nCreating app recency segments...\")\n",
    "df_engineered['App_Recency_Segment'] = pd.cut(\n",
    "    df_engineered['QT_DAYS_LAST_LOGIN_APP_MYFID'].fillna(999),\n",
    "    bins=[-1, 30, 90, 180, 360, 999],\n",
    "    labels=['Very_Recent', 'Recent', 'Moderate', 'Old', 'Very_Old']\n",
    ")\n",
    "\n",
    "recency_analysis = df_engineered.groupby('App_Recency_Segment')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "recency_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "print(\"App recency vs Target:\")\n",
    "print(recency_analysis.sort_values('Conversion_Rate', ascending=False))"
   ],
   "id": "58c052c59d1ae69c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 14 - Digital Engagement Segmentation:\n",
      "==================================================\n",
      "Digital engagement segments vs Target:\n",
      "                    Customers  MySavings_Buyers  Conversion_Rate\n",
      "Digital_Engagement                                              \n",
      "High_Digital            22687                87           0.0038\n",
      "Medium_Digital         124994               286           0.0023\n",
      "Low_Digital            230202               330           0.0014\n",
      "No_Digital                  5                 0           0.0000\n",
      "\n",
      "Creating app recency segments...\n",
      "App recency vs Target:\n",
      "                     Customers  MySavings_Buyers  Conversion_Rate\n",
      "App_Recency_Segment                                              \n",
      "Very_Recent              86504               212           0.0025\n",
      "Recent                    5663                11           0.0019\n",
      "Old                     100729               190           0.0019\n",
      "Moderate                  7572                14           0.0018\n",
      "Very_Old                177420               276           0.0016\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:01:05.173386Z",
     "start_time": "2025-06-02T10:01:05.103659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 15 - Feature Engineering Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all new features created\n",
    "new_features = [\n",
    "    'Age_Group', 'Total_Insurance_Products', 'CLV_numeric', 'CLV_Segment',\n",
    "    'Financial_Health_Score', 'Digital_Channels_Count', 'Digital_Engagement',\n",
    "    'App_Recency_Segment'\n",
    "]\n",
    "\n",
    "print(\"New features created:\")\n",
    "for feature in new_features:\n",
    "    if feature in df_engineered.columns:\n",
    "        print(f\"  ✓ {feature}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {feature} (missing)\")\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {df_engineered.shape}\")\n",
    "\n",
    "# Show top converting segments across all new features\n",
    "print(f\"\\nTop converting segments (>0.003% conversion rate):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Age groups\n",
    "top_age = df_engineered.groupby('Age_Group')['Target'].mean().sort_values(ascending=False)\n",
    "print(f\"Best age group: {top_age.index[0]} ({top_age.iloc[0]:.4f})\")\n",
    "\n",
    "# Digital engagement\n",
    "top_digital = df_engineered.groupby('Digital_Engagement')['Target'].mean().sort_values(ascending=False)\n",
    "print(f\"Best digital engagement: {top_digital.index[0]} ({top_digital.iloc[0]:.4f})\")\n",
    "\n",
    "# Financial health score\n",
    "top_financial = df_engineered.groupby('Financial_Health_Score')['Target'].mean().sort_values(ascending=False)\n",
    "print(f\"Best financial score: {top_financial.index[0]} ({top_financial.iloc[0]:.4f})\")"
   ],
   "id": "c74ca6326fc3fca7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 15 - Feature Engineering Summary:\n",
      "============================================================\n",
      "New features created:\n",
      "  ✓ Age_Group\n",
      "  ✓ Total_Insurance_Products\n",
      "  ✓ CLV_numeric\n",
      "  ✓ CLV_Segment\n",
      "  ✓ Financial_Health_Score\n",
      "  ✓ Digital_Channels_Count\n",
      "  ✓ Digital_Engagement\n",
      "  ✓ App_Recency_Segment\n",
      "\n",
      "Dataset shape after feature engineering: (377888, 565)\n",
      "\n",
      "Top converting segments (>0.003% conversion rate):\n",
      "--------------------------------------------------\n",
      "Best age group: 18-24 (0.0037)\n",
      "Best digital engagement: High_Digital (0.0038)\n",
      "Best financial score: 11.0 (0.0023)\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:05:21.008372Z",
     "start_time": "2025-06-02T10:05:20.597446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 16 - Product Combination Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze which insurance products are most common together\n",
    "insurance_product_cols = [col for col in df_engineered.columns if 'QTD_APL_VIG_T_' in col and col != 'QTD_APL_VIG_T_S']\n",
    "\n",
    "print(f\"Found {len(insurance_product_cols)} specific insurance products\")\n",
    "\n",
    "# Show first few to understand the product types\n",
    "for col in insurance_product_cols[:8]:\n",
    "    non_zero = (df_engineered[col] > 0).sum()\n",
    "    print(f\"  {col}: {non_zero:,} customers ({non_zero / len(df_engineered) * 100:.1f}%)\")\n",
    "\n",
    "# Create product diversity score\n",
    "print(f\"\\nCreating product diversity features...\")\n",
    "\n",
    "# Count how many different product types each customer has\n",
    "df_engineered['Product_Diversity'] = (df_engineered[insurance_product_cols] > 0).sum(axis=1)\n",
    "\n",
    "# Create product concentration (is portfolio concentrated in few products?)\n",
    "total_products = df_engineered[insurance_product_cols].sum(axis=1)\n",
    "max_single_product = df_engineered[insurance_product_cols].max(axis=1)\n",
    "df_engineered['Product_Concentration'] = max_single_product / (total_products + 0.01)  # Avoid division by zero\n",
    "\n",
    "print(\"Product diversity vs Target:\")\n",
    "diversity_analysis = df_engineered.groupby('Product_Diversity')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "diversity_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "# Show only groups with sufficient data\n",
    "diversity_analysis = diversity_analysis[diversity_analysis['Customers'] >= 100]\n",
    "print(diversity_analysis.head(10))"
   ],
   "id": "56d0690ddea628fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 16 - Product Combination Analysis:\n",
      "==================================================\n",
      "Found 19 specific insurance products\n",
      "  QTD_APL_VIG_T_S_AP: 21,474 customers (5.7%)\n",
      "  QTD_APL_VIG_T_S_ATCP: 10,714 customers (2.8%)\n",
      "  QTD_APL_VIG_T_S_CAP: 62,926 customers (16.7%)\n",
      "  QTD_APL_VIG_T_S_PPR: 68,284 customers (18.1%)\n",
      "  QTD_APL_VIG_T_S_SD: 68,123 customers (18.0%)\n",
      "  QTD_APL_VIG_T_S_VDR: 26,226 customers (6.9%)\n",
      "  QTD_APL_VIG_T_AP: 23,875 customers (6.3%)\n",
      "  QTD_APL_VIG_T_ATCO: 10,038 customers (2.7%)\n",
      "\n",
      "Creating product diversity features...\n",
      "Product diversity vs Target:\n",
      "                   Customers  MySavings_Buyers  Conversion_Rate\n",
      "Product_Diversity                                              \n",
      "0                        933                 0           0.0000\n",
      "1                     135803               141           0.0010\n",
      "2                      91903               133           0.0014\n",
      "3                      64512               148           0.0023\n",
      "4                      35656                98           0.0027\n",
      "5                      21485                68           0.0032\n",
      "6                      12677                53           0.0042\n",
      "7                       6795                25           0.0037\n",
      "8                       3936                18           0.0046\n",
      "9                       2024                 8           0.0040\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:06:04.094699Z",
     "start_time": "2025-06-02T10:05:52.761747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 17 - Time-Based Behavior Patterns:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find recency columns for different behaviors\n",
    "recency_cols = [col for col in df_engineered.columns if 'QTD_MESES_ULT' in col]\n",
    "print(f\"Found {len(recency_cols)} recency columns\")\n",
    "\n",
    "# Show key recency patterns\n",
    "for col in recency_cols[:5]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Non-null: {df_engineered[col].notna().sum()}\")\n",
    "    if df_engineered[col].notna().sum() > 0:\n",
    "        print(f\"  Distribution: {df_engineered[col].describe()}\")\n",
    "\n",
    "# Create customer lifecycle stage based on tenure and recent activity\n",
    "print(f\"\\nCreating customer lifecycle features...\")\n",
    "\n",
    "# Recent activity score (lower = more recent activity)\n",
    "if 'QTD_MESES_ULT_COMPRA_APL' in df_engineered.columns:\n",
    "    df_engineered['Recent_Purchase_Activity'] = df_engineered['QTD_MESES_ULT_COMPRA_APL'].fillna(999)\n",
    "\n",
    "\n",
    "    # Create lifecycle segments\n",
    "    def create_lifecycle_stage(tenure, recent_activity):\n",
    "        if tenure < 2:\n",
    "            return 'New_Customer'\n",
    "        elif recent_activity <= 6:\n",
    "            return 'Active_Customer'\n",
    "        elif recent_activity <= 24:\n",
    "            return 'Moderate_Customer'\n",
    "        else:\n",
    "            return 'Dormant_Customer'\n",
    "\n",
    "\n",
    "    df_engineered['Lifecycle_Stage'] = df_engineered.apply(\n",
    "        lambda x: create_lifecycle_stage(x['QTD_ANOS_ANTIG_CLIENTE'], x['Recent_Purchase_Activity']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    lifecycle_analysis = df_engineered.groupby('Lifecycle_Stage')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    lifecycle_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "    print(\"Customer lifecycle vs Target:\")\n",
    "    print(lifecycle_analysis.sort_values('Conversion_Rate', ascending=False))"
   ],
   "id": "2ff86cdf2ffe6e2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 17 - Time-Based Behavior Patterns:\n",
      "==================================================\n",
      "Found 24 recency columns\n",
      "\n",
      "QTD_MESES_ULT_CONT:\n",
      "  Non-null: 377888\n",
      "  Distribution: count    377888.000000\n",
      "mean         13.268863\n",
      "std          15.465711\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           8.000000\n",
      "75%          21.000000\n",
      "max         200.000000\n",
      "Name: QTD_MESES_ULT_CONT, dtype: float64\n",
      "\n",
      "QTD_MESES_ULT_COMPRA_APL:\n",
      "  Non-null: 377888\n",
      "  Distribution: count    377888.000000\n",
      "mean         46.720150\n",
      "std          49.448658\n",
      "min          -1.000000\n",
      "25%          12.000000\n",
      "50%          31.000000\n",
      "75%          63.000000\n",
      "max        1225.000000\n",
      "Name: QTD_MESES_ULT_COMPRA_APL, dtype: float64\n",
      "\n",
      "QTD_MESES_ULT_COMPRA_AP:\n",
      "  Non-null: 97575\n",
      "  Distribution: count    97575.000000\n",
      "mean       170.636618\n",
      "std        130.199750\n",
      "min          0.000000\n",
      "25%         49.000000\n",
      "50%        153.000000\n",
      "75%        274.000000\n",
      "max       1027.000000\n",
      "Name: QTD_MESES_ULT_COMPRA_AP, dtype: float64\n",
      "\n",
      "QTD_MESES_ULT_COMPRA_ATCO:\n",
      "  Non-null: 22188\n",
      "  Distribution: count    22188.000000\n",
      "mean       141.878222\n",
      "std        129.139914\n",
      "min          0.000000\n",
      "25%         51.000000\n",
      "50%        119.000000\n",
      "75%        195.000000\n",
      "max       1485.000000\n",
      "Name: QTD_MESES_ULT_COMPRA_ATCO, dtype: float64\n",
      "\n",
      "QTD_MESES_ULT_COMPRA_ATCP:\n",
      "  Non-null: 26885\n",
      "  Distribution: count    26885.000000\n",
      "mean       108.775935\n",
      "std        101.419589\n",
      "min          0.000000\n",
      "25%         35.000000\n",
      "50%         78.000000\n",
      "75%        150.000000\n",
      "max       1485.000000\n",
      "Name: QTD_MESES_ULT_COMPRA_ATCP, dtype: float64\n",
      "\n",
      "Creating customer lifecycle features...\n",
      "Customer lifecycle vs Target:\n",
      "                   Customers  MySavings_Buyers  Conversion_Rate\n",
      "Lifecycle_Stage                                                \n",
      "Active_Customer        47389               126           0.0027\n",
      "Moderate_Customer      95527               219           0.0023\n",
      "New_Customer           13970                22           0.0016\n",
      "Dormant_Customer      221002               336           0.0015\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:08:38.147352Z",
     "start_time": "2025-06-02T10:08:37.884487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 18 - Geographic Intelligence:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find geographic columns\n",
    "geo_cols = [col for col in df_engineered.columns if\n",
    "            any(x in col.upper() for x in ['DISTRITO', 'CONCELHO', 'NUT', 'RURAL', 'LITORAL'])]\n",
    "print(f\"Found {len(geo_cols)} geographic columns\")\n",
    "\n",
    "# Check what geographic data we have\n",
    "key_geo_cols = ['DSC_DISTRITO', 'DSC_CONCELHO', 'DSC_RURAL_URBANO', 'DSC_INTERIOR_LITORAL']\n",
    "available_geo = [col for col in key_geo_cols if col in df_engineered.columns]\n",
    "\n",
    "print(f\"Available geographic columns: {available_geo}\")\n",
    "\n",
    "for col in available_geo:\n",
    "    print(f\"\\n{col}:\")\n",
    "    top_values = df_engineered[col].value_counts().head(5)\n",
    "    print(f\"  Top values: {top_values.to_dict()}\")\n",
    "\n",
    "# Create geographic risk segments\n",
    "if 'DSC_RURAL_URBANO' in df_engineered.columns:\n",
    "    print(f\"\\nUrban vs Rural analysis:\")\n",
    "    urban_rural_analysis = df_engineered.groupby('DSC_RURAL_URBANO')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    urban_rural_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "    print(urban_rural_analysis)\n",
    "\n",
    "if 'DSC_INTERIOR_LITORAL' in df_engineered.columns:\n",
    "    print(f\"\\nInterior vs Coastal analysis:\")\n",
    "    interior_analysis = df_engineered.groupby('DSC_INTERIOR_LITORAL')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    interior_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "    print(interior_analysis)\n",
    "\n",
    "# Create district-level conversion rates (for district encoding)\n",
    "if 'DSC_DISTRITO' in df_engineered.columns:\n",
    "    print(f\"\\nTop converting districts:\")\n",
    "    district_analysis = df_engineered.groupby('DSC_DISTRITO')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    district_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "    district_analysis = district_analysis[district_analysis['Customers'] >= 1000]  # Only districts with sufficient data\n",
    "    district_analysis = district_analysis.sort_values('Conversion_Rate', ascending=False)\n",
    "    print(district_analysis.head(10))\n",
    "\n",
    "    # Create district performance encoding\n",
    "    district_performance = district_analysis['Conversion_Rate'].to_dict()\n",
    "    df_engineered['District_Performance'] = df_engineered['DSC_DISTRITO'].map(district_performance).fillna(\n",
    "        df_engineered['Target'].mean())"
   ],
   "id": "503417e81050ea72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 18 - Geographic Intelligence:\n",
      "==================================================\n",
      "Found 10 geographic columns\n",
      "Available geographic columns: ['DSC_DISTRITO', 'DSC_CONCELHO', 'DSC_RURAL_URBANO', 'DSC_INTERIOR_LITORAL']\n",
      "\n",
      "DSC_DISTRITO:\n",
      "  Top values: {'LISBOA': 103950, 'PORTO': 58709, 'SETÚBAL': 34722, 'BRAGA': 26380, 'AVEIRO': 25415}\n",
      "\n",
      "DSC_CONCELHO:\n",
      "  Top values: {'LISBOA': 28077, 'SINTRA': 16027, 'CASCAIS': 10288, 'OEIRAS': 10201, 'VILA NOVA DE GAIA': 9470}\n",
      "\n",
      "DSC_RURAL_URBANO:\n",
      "  Top values: {'ÁREA PREDOMINANTEMENTE URBANA': 191911, 'ÁREA MEDIAMENTE URBANA': 119304, 'ÁREA PREDOMINANTEMENTE RURAL': 65453}\n",
      "\n",
      "DSC_INTERIOR_LITORAL:\n",
      "  Top values: {'INTERIOR': 199629, 'LITORAL': 177039}\n",
      "\n",
      "Urban vs Rural analysis:\n",
      "                               Customers  MySavings_Buyers  Conversion_Rate\n",
      "DSC_RURAL_URBANO                                                           \n",
      "ÁREA MEDIAMENTE URBANA            119304               260           0.0022\n",
      "ÁREA PREDOMINANTEMENTE RURAL       65453               126           0.0019\n",
      "ÁREA PREDOMINANTEMENTE URBANA     191911               317           0.0017\n",
      "\n",
      "Interior vs Coastal analysis:\n",
      "                      Customers  MySavings_Buyers  Conversion_Rate\n",
      "DSC_INTERIOR_LITORAL                                              \n",
      "INTERIOR                 199629               406           0.0020\n",
      "LITORAL                  177039               297           0.0017\n",
      "\n",
      "Top converting districts:\n",
      "                  Customers  MySavings_Buyers  Conversion_Rate\n",
      "DSC_DISTRITO                                                  \n",
      "GUARDA                 3897                16           0.0041\n",
      "COIMBRA               17444                44           0.0025\n",
      "PORTO                 58709               146           0.0025\n",
      "LEIRIA                14756                35           0.0024\n",
      "VIANA DO CASTELO       6841                16           0.0023\n",
      "BRAGA                 26380                57           0.0022\n",
      "SANTARÉM              18302                40           0.0022\n",
      "PORTALEGRE             3361                 7           0.0021\n",
      "ILHA TERCEIRA          1022                 2           0.0020\n",
      "ÉVORA                  5433                11           0.0020\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:09:04.470967Z",
     "start_time": "2025-06-02T10:09:04.291268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 19 - Advanced Interaction Features:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create powerful interaction features based on our best predictors\n",
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# Age × Digital engagement interaction\n",
    "df_engineered['Age_Digital_Score'] = df_engineered['QTD_IDADE'] * df_engineered['Digital_Channels_Count']\n",
    "\n",
    "# Financial health × Product diversity interaction\n",
    "df_engineered['Financial_Product_Score'] = df_engineered['Financial_Health_Score'] * df_engineered['Product_Diversity']\n",
    "\n",
    "# Tenure × Recent activity interaction (engagement decay)\n",
    "df_engineered['Tenure_Activity_Ratio'] = df_engineered['QTD_ANOS_ANTIG_CLIENTE'] / (\n",
    "            df_engineered['Recent_Purchase_Activity'] + 1)\n",
    "\n",
    "# High-value digital customer flag\n",
    "df_engineered['High_Value_Digital'] = (\n",
    "        (df_engineered['Digital_Engagement'] == 'High_Digital') &\n",
    "        (df_engineered['CLV_Segment'] == 'Premium')\n",
    ").astype(int)\n",
    "\n",
    "# Young active customer flag\n",
    "df_engineered['Young_Active_Customer'] = (\n",
    "        (df_engineered['Age_Group'] == '18-24') &\n",
    "        (df_engineered['Lifecycle_Stage'] == 'Active_Customer')\n",
    ").astype(int)\n",
    "\n",
    "print(\"Analyzing interaction features:\")\n",
    "\n",
    "# Test high-value digital customers\n",
    "if df_engineered['High_Value_Digital'].sum() > 0:\n",
    "    hvd_analysis = df_engineered.groupby('High_Value_Digital')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    hvd_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "    print(\"High Value Digital customers:\")\n",
    "    print(hvd_analysis)\n",
    "\n",
    "# Test young active customers\n",
    "if df_engineered['Young_Active_Customer'].sum() > 0:\n",
    "    yac_analysis = df_engineered.groupby('Young_Active_Customer')['Target'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    yac_analysis.columns = ['Customers', 'MySavings_Buyers', 'Conversion_Rate']\n",
    "    print(\"\\nYoung Active customers:\")\n",
    "    print(yac_analysis)\n",
    "\n",
    "# Show correlation of new interaction features with target\n",
    "interaction_features = ['Age_Digital_Score', 'Financial_Product_Score', 'Tenure_Activity_Ratio', 'High_Value_Digital',\n",
    "                        'Young_Active_Customer']\n",
    "print(f\"\\nCorrelation with Target:\")\n",
    "for feature in interaction_features:\n",
    "    if feature in df_engineered.columns:\n",
    "        corr = df_engineered[feature].corr(df_engineered['Target'])\n",
    "        print(f\"  {feature}: {corr:.4f}\")"
   ],
   "id": "58157657ee68dc82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 19 - Advanced Interaction Features:\n",
      "==================================================\n",
      "Creating interaction features...\n",
      "Analyzing interaction features:\n",
      "High Value Digital customers:\n",
      "                    Customers  MySavings_Buyers  Conversion_Rate\n",
      "High_Value_Digital                                              \n",
      "0                      372017               680           0.0018\n",
      "1                        5871                23           0.0039\n",
      "\n",
      "Young Active customers:\n",
      "                       Customers  MySavings_Buyers  Conversion_Rate\n",
      "Young_Active_Customer                                              \n",
      "0                         377020               699           0.0019\n",
      "1                            868                 4           0.0046\n",
      "\n",
      "Correlation with Target:\n",
      "  Age_Digital_Score: 0.0104\n",
      "  Financial_Product_Score: 0.0158\n",
      "  Tenure_Activity_Ratio: nan\n",
      "  High_Value_Digital: 0.0060\n",
      "  Young_Active_Customer: 0.0031\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:11:11.022558Z",
     "start_time": "2025-06-02T10:11:10.956385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 20 - Advanced Feature Engineering Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all advanced features created\n",
    "advanced_features = [\n",
    "    'Product_Diversity', 'Product_Concentration', 'Lifecycle_Stage',\n",
    "    'District_Performance', 'Age_Digital_Score', 'Financial_Product_Score',\n",
    "    'High_Value_Digital', 'Young_Active_Customer'\n",
    "]\n",
    "\n",
    "print(\"Advanced features created:\")\n",
    "for feature in advanced_features:\n",
    "    if feature in df_engineered.columns:\n",
    "        print(f\"  ✓ {feature}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {feature} (missing)\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFinal dataset shape: {df_engineered.shape}\")\n",
    "print(f\"Total features available: {df_engineered.shape[1] - 1}\")  # Minus target\n",
    "\n",
    "# Top predictive features summary\n",
    "print(f\"\\nTop predictive patterns discovered:\")\n",
    "print(f\"  🏆 Young Active Customers: 0.46% conversion (2.4x baseline)\")\n",
    "print(f\"  🏆 High Value Digital: 0.39% conversion (2.2x baseline)\")\n",
    "print(f\"  🏆 Product Diversity 8+: 0.46% conversion (4.6x baseline)\")\n",
    "print(f\"  🏆 High Digital Engagement: 0.38% conversion (2.0x baseline)\")\n",
    "print(f\"  🏆 Age 18-24: 0.37% conversion (1.9x baseline)\")\n",
    "\n",
    "# Check for any missing values in key engineered features\n",
    "print(f\"\\nMissing values in engineered features:\")\n",
    "key_engineered = ['Age_Group', 'Digital_Engagement', 'Lifecycle_Stage', 'CLV_Segment', 'Product_Diversity']\n",
    "for feature in key_engineered:\n",
    "    if feature in df_engineered.columns:\n",
    "        missing_pct = (df_engineered[feature].isnull().sum() / len(df_engineered)) * 100\n",
    "        print(f\"  {feature}: {missing_pct:.1f}%\")"
   ],
   "id": "59af00efaf928843",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 20 - Advanced Feature Engineering Summary:\n",
      "============================================================\n",
      "Advanced features created:\n",
      "  ✓ Product_Diversity\n",
      "  ✓ Product_Concentration\n",
      "  ✓ Lifecycle_Stage\n",
      "  ✓ District_Performance\n",
      "  ✓ Age_Digital_Score\n",
      "  ✓ Financial_Product_Score\n",
      "  ✓ High_Value_Digital\n",
      "  ✓ Young_Active_Customer\n",
      "\n",
      "Final dataset shape: (377888, 575)\n",
      "Total features available: 574\n",
      "\n",
      "Top predictive patterns discovered:\n",
      "  🏆 Young Active Customers: 0.46% conversion (2.4x baseline)\n",
      "  🏆 High Value Digital: 0.39% conversion (2.2x baseline)\n",
      "  🏆 Product Diversity 8+: 0.46% conversion (4.6x baseline)\n",
      "  🏆 High Digital Engagement: 0.38% conversion (2.0x baseline)\n",
      "  🏆 Age 18-24: 0.37% conversion (1.9x baseline)\n",
      "\n",
      "Missing values in engineered features:\n",
      "  Age_Group: 0.0%\n",
      "  Digital_Engagement: 0.0%\n",
      "  Lifecycle_Stage: 0.0%\n",
      "  CLV_Segment: 7.3%\n",
      "  Product_Diversity: 0.0%\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:11:21.510754Z",
     "start_time": "2025-06-02T10:11:21.471990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"STEP 21 - Ready for Modeling Assessment:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Categorize all our features for modeling prep\n",
    "feature_types = {\n",
    "    'Target': ['Target'],\n",
    "    'ID_Columns': ['ID_CLIENTE_ANON', 'ANO_MES'],\n",
    "    'Original_Numeric': [],\n",
    "    'Original_Categorical': [],\n",
    "    'Engineered_Numeric': ['Total_Insurance_Products', 'CLV_numeric', 'Financial_Health_Score',\n",
    "                           'Digital_Channels_Count', 'Product_Diversity', 'Age_Digital_Score',\n",
    "                           'Financial_Product_Score'],\n",
    "    'Engineered_Categorical': ['Age_Group', 'CLV_Segment', 'Digital_Engagement',\n",
    "                               'App_Recency_Segment', 'Lifecycle_Stage'],\n",
    "    'Engineered_Binary': ['High_Value_Digital', 'Young_Active_Customer']\n",
    "}\n",
    "\n",
    "# Count features in each category\n",
    "for category, features in feature_types.items():\n",
    "    available = [f for f in features if f in df_engineered.columns]\n",
    "    print(f\"{category}: {len(available)} features\")\n",
    "\n",
    "# Check if we're ready for modeling\n",
    "modeling_readiness = {\n",
    "    'Target_Variable': 'Target' in df_engineered.columns,\n",
    "    'Numeric_Features': len([f for f in feature_types['Engineered_Numeric'] if f in df_engineered.columns]) > 0,\n",
    "    'Categorical_Features': len([f for f in feature_types['Engineered_Categorical'] if f in df_engineered.columns]) > 0,\n",
    "    'No_Major_Missing': all(df_engineered[f].isnull().sum() < len(df_engineered) * 0.5 for f in\n",
    "                            ['Target', 'Age_Group', 'Digital_Engagement'])\n",
    "}\n",
    "\n",
    "print(f\"\\nModeling readiness check:\")\n",
    "for check, status in modeling_readiness.items():\n",
    "    status_icon = \"✅\" if status else \"❌\"\n",
    "    print(f\"  {status_icon} {check}\")\n",
    "\n",
    "all_ready = all(modeling_readiness.values())\n",
    "print(f\"\\n{'🚀 READY FOR MODELING!' if all_ready else '⚠️ Need to fix issues before modeling'}\")"
   ],
   "id": "1cc3c878ad76e33e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 21 - Ready for Modeling Assessment:\n",
      "============================================================\n",
      "Target: 1 features\n",
      "ID_Columns: 1 features\n",
      "Original_Numeric: 0 features\n",
      "Original_Categorical: 0 features\n",
      "Engineered_Numeric: 7 features\n",
      "Engineered_Categorical: 5 features\n",
      "Engineered_Binary: 2 features\n",
      "\n",
      "Modeling readiness check:\n",
      "  ✅ Target_Variable\n",
      "  ✅ Numeric_Features\n",
      "  ✅ Categorical_Features\n",
      "  ✅ No_Major_Missing\n",
      "\n",
      "🚀 READY FOR MODELING!\n"
     ]
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
